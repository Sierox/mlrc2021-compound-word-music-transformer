{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet#: Knet, minibatch, param, param0, softmax, nll, bmm, relu, dropout, progress!, adam\n",
    "using Knet: AutoGrad.@primitive\n",
    "using CUDA: CuArray\n",
    "using Pickle\n",
    "using NPZ\n",
    "using Statistics: mean, std\n",
    "using StatsBase: sample, Weights\n",
    "using DataStructures: OrderedDict\n",
    "using LinearAlgebra\n",
    "using OMEinsum\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### Base Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Linear; W; b; end\n",
    "Linear(i::Int, o...) =\n",
    "    Linear(param(o..., i, atype=ATYPE), param0(o..., atype=ATYPE))\n",
    "(l::Linear)(x) =\n",
    "    reshape(reshape(l.W,:,size(l.W)[end]) * reshape(x,size(x)[1],:), size(l.W)[1:end-1]..., size(x)[2:end]...) .+ l.b\n",
    "\n",
    "struct Activation; f; o; end\n",
    "Activation(f, o...) = \n",
    "    Activation(f, o)\n",
    "(a::Activation)(x) =\n",
    "    a.f.(x, a.o...)\n",
    "\n",
    "struct Embedding; W; end\n",
    "Embedding(n_ttypes::Int, embed::Int) =\n",
    "    Embedding(param(embed, n_ttypes, atype=ATYPE))\n",
    "(e::Embedding)(x) = \n",
    "    e.W[:, x] * √size(e.W, 1)\n",
    "\n",
    "struct Dropout; p; end\n",
    "(d::Dropout)(x) =\n",
    "    dropout(x, d.p)\n",
    "\n",
    "struct LayerNorm; W; b; ϵ; end\n",
    "LayerNorm(d_model; eps=1e-6) = \n",
    "    LayerNorm(param(d_model; init=ones, atype=ATYPE),\n",
    "              param(d_model, init=zeros, atype=ATYPE),\n",
    "              eps)\n",
    "function (ln::LayerNorm)(x, o...; dims=1)\n",
    "    μ = mean(x, dims=dims)\n",
    "    σ = std(x, mean=μ, dims=dims, corrected=false)\n",
    "    ln.W .* (x .- μ) ./ (σ .+ eltype(x)(ln.ϵ)) .+ ln.b\n",
    "end\n",
    "\n",
    "struct PositionalEncoding; W; dropout; end\n",
    "function PositionalEncoding(d_model::Int; dropout=0.1, max_len=20000)\n",
    "    W = zeros(d_model, max_len)\n",
    "    pos = exp.((0:2:d_model-1) .* (-log(10000)/d_model)) * (0:max_len-1)'\n",
    "    W[1:2:end, :] = sin.(pos)\n",
    "    W[2:2:end, :] = cos.(pos)\n",
    "    PositionalEncoding(ATYPE(W), Dropout(dropout))\n",
    "end\n",
    "function (pe::PositionalEncoding)(x)\n",
    "    pe.dropout(x .+ pe.W[:, 1:size(x,2), :])\n",
    "end\n",
    "\n",
    "struct Chain; layers; end\n",
    "Chain(l, l_...) =\n",
    "    Chain(append!(Vector{Any}([l]), l_))\n",
    "function (c::Chain)(x, o...; state=nothing)\n",
    "    training = state == nothing\n",
    "    for layer in c.layers\n",
    "        if (typeof(layer) != TransformerEncoder) || training\n",
    "            x = layer(x, o...)\n",
    "        else\n",
    "            x, state = layer(x, o..., state=state)\n",
    "        end\n",
    "    end\n",
    "    training ? x : (x, state)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/2006.16236.pdf\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/attention_layer.py\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/recurrent/attention/self_attention/attention_layer.py\n",
    "struct Attention; q; k; v; o; inner; masker; end\n",
    "Attention(d_model::Int, n_heads::Int, inner, masker) =\n",
    "    Attention(Linear(d_model, d_model÷n_heads, n_heads),\n",
    "              Linear(d_model, d_model÷n_heads, n_heads),\n",
    "              Linear(d_model, d_model÷n_heads, n_heads),\n",
    "              Linear(d_model, d_model),\n",
    "              inner, masker)\n",
    "function (a::Attention)(q, k, v; state=nothing)\n",
    "    training = state==nothing\n",
    "    if training\n",
    "        _, T, B = size(q); H = size(a.q.W, 2)\n",
    "\n",
    "        q = reshape(a.q(q), (T, :, H, B))\n",
    "        k = reshape(a.k(k), (T, :, H, B))\n",
    "        v = reshape(a.v(v), (T, :, H, B))\n",
    "\n",
    "        o = a.inner(q, k, v, mask=a.masker(T))\n",
    "        return a.o(reshape(o, (:, T, B)))\n",
    "    else\n",
    "        _, _, B = size(q); H = size(a.q.W, 2)\n",
    "\n",
    "        q = reshape(a.q(q), (B, H, :))\n",
    "        k = reshape(a.k(k), (B, H, :))\n",
    "        v = reshape(a.v(v), (B, H, :))\n",
    "    \n",
    "        v_, state = a.inner(q, k, v, state=state)\n",
    "        o = a.o(reshape(v_, (:, B)))\n",
    "        return o, state\n",
    "    end\n",
    "end\n",
    "(a::Attention)(x; state=nothing) = a(x, x, x, state=state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Linear Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lower_triangular_mask (generic function with 1 method)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://arxiv.org/pdf/2006.16236.pdf\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/causal_linear_attention.py\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/recurrent/attention/self_attention/linear_attention.py\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/causal_product/causal_product_cpu.cpp\n",
    "function causal_linear_attention(q, k, v; ϕ=gelu, ϵ=1e-6, mask=nothing, state=nothing)\n",
    "    training = state==nothing\n",
    "    if training\n",
    "        Q = ϕ.(q)\n",
    "        K = ϕ.(k)\n",
    "        K_ = permutedims(cumsum(permutedims(K, [2,1,3,4]), dims=2), [2,1,3,4])\n",
    "        Z = 1 ./ (sum(Q .* K_, dims=2) .+ ϵ)\n",
    "        S = bmm(K, v, transB=true)\n",
    "        S = S .* mask\n",
    "        QS = bmm(Q, S, transA=true)\n",
    "        V = permutedims(QS, [2,1,3,4])\n",
    "        return V .* Z\n",
    "    else\n",
    "        Q = ϕ.(q)\n",
    "        K = ϕ.(k)\n",
    "        if state==\"init\"\n",
    "            Si = ATYPE(zeros(size(Q)..., size(v,3)))\n",
    "            Zi = ATYPE(zeros(size(Q)...))\n",
    "        else\n",
    "            Si, Zi = state\n",
    "        end\n",
    "        @assert size(Si, 1) == size(Q, 1)\n",
    "        Zi = Zi .+ K\n",
    "        Si = Si .+ ein\"bhd, bhm -> bhdm\"(K, v)\n",
    "        Z = 1 ./ (ein\"bhd, bhd -> bh\"(Q, Zi) .+ ϵ)\n",
    "        V = ein\"bhd, bhdm, bh -> bhm\"(Q, Si, Z)\n",
    "        return V, (Si, Zi)\n",
    "    end\n",
    "end\n",
    "\n",
    "gelu(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x^3)));\n",
    "lower_triangular_mask(N) = ATYPE(Matrix{Float32}(UpperTriangular(ones(N,N))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/2006.16236.pdf\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/transformers.py\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/recurrent/transformers.py\n",
    "struct TransformerEncoder; attention; drop; norm1; fc; norm2; end\n",
    "function TransformerEncoder(d_model::Int, d_ff::Int, n_heads::Int, dropout, activation, aparams...)\n",
    "    TransformerEncoder(\n",
    "        Attention(d_model, n_heads, causal_linear_attention, lower_triangular_mask),\n",
    "        Dropout(dropout),\n",
    "        LayerNorm(d_model),\n",
    "        Chain(Linear(d_model, d_ff), Activation(activation, aparams...), Dropout(dropout),\n",
    "            Linear(d_ff, d_model), Dropout(dropout)),\n",
    "        LayerNorm(d_model))\n",
    "end\n",
    "function (enc::TransformerEncoder)(x; state=nothing)\n",
    "    training = state == nothing\n",
    "    if training\n",
    "        x_ = enc.attention(x)\n",
    "        x = x .+ enc.drop(x_)\n",
    "        y = x = enc.norm1(x)\n",
    "        y = enc.fc(y)\n",
    "        return enc.norm2(x+y)\n",
    "    else\n",
    "        x_, state = enc.attention(x, state=state)\n",
    "        x = x .+ enc.drop(x_)\n",
    "        y = x = enc.norm1(x)\n",
    "        y = enc.fc(y)\n",
    "        return (enc.norm2(x+y), state)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nucleus (generic function with 1 method)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sampling(x; t=1.0, p=nothing)\n",
    "    x = Vector{Float64}(reshape(x, length(x)))\n",
    "    x = softmax(x./t)\n",
    "    x = p!=nothing ? nucleus(x, p) : x\n",
    "    c = sample(1:length(x), Weights(x))\n",
    "    return c\n",
    "end\n",
    "\n",
    "function nucleus(x, p; ϵ=1e-5)\n",
    "    x = x./(sum(x)+ϵ)\n",
    "    i = sortperm(x, rev=true)\n",
    "    x_ = cumsum(sort(x, rev=true)).>p\n",
    "    c = sum(x_)>0 ? i[1:findfirst(x_)] : i\n",
    "    x = [i in c ? x[i] : 0 for i in 1:length(x)]\n",
    "    x = x./sum(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound Word Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CPTransformer; embeds; fc_in; posenc; enc; projs; proj_blend; end\n",
    "\n",
    "CPTransformer(n_ttypes::Vector{Int}, embed_sizes::Vector{Int}, d_model::Int, d_inner::Int, n_heads::Int, n_layers::Int, dropout) =\n",
    "    CPTransformer([Embedding(n, e) for (n, e) in zip(n_ttypes, embed_sizes)],\n",
    "            Linear(sum(embed_sizes), d_model),\n",
    "            PositionalEncoding(d_model),\n",
    "            Chain([TransformerEncoder(d_model, d_inner, n_heads, dropout, relu) for n=1:n_layers]..., LayerNorm(d_model)),\n",
    "            [Linear(d_model, n) for n in n_ttypes],\n",
    "            Linear(d_model + embed_sizes[ttypeids[\"type\"]], d_model))\n",
    "\n",
    "function (model::CPTransformer)(x; y=nothing, state=nothing)\n",
    "        training = y!=nothing\n",
    "\n",
    "        if training\n",
    "                x, loss_mask = x[:, 1:end-1, :], ATYPE(x[:, end, :].-1);        #@show size(x) # (T, N, B)\n",
    "        elseif state==nothing\n",
    "                state=\"init\"\n",
    "        end\n",
    "\n",
    "        x = vcat([embed(x[:, i, :]) for (embed, i) in\n",
    "                zip(model.embeds, 1:length(model.embeds))]...);                 #@show size(x) # (D_e, T, B)\n",
    "        \n",
    "        x = model.fc_in(x);                                                     #@show size(x) # (D_i, T, B)\n",
    "\n",
    "        x = model.posenc(x);                                                    #@show size(x) # (D_i, T, B)\n",
    "\n",
    "        if training \n",
    "                h = model.enc(x)\n",
    "        else \n",
    "                h, state = model.enc(x, state=state)\n",
    "        end;                                                                    #@show size(h) # (D_i, T, B)\n",
    "\n",
    "        ŷ_t_P = model.projs[ttypeids[\"type\"]](h);                               #@show size(ŷ_t_P) # (N_t, T, B)\n",
    "\n",
    "        ŷ_t = training ?\n",
    "                y[:, ttypeids[\"type\"], :] : \n",
    "                sampling(ŷ_t_P, p=0.9);                                         #@show size(ŷ_t) # (T, B)\n",
    "                \n",
    "        ŷ_τ = vcat([h, model.embeds[ttypeids[\"type\"]](ŷ_t)]...);                #@show size(ŷ_τ) # (D_i + D_t, T, B)\n",
    "\n",
    "        h_ = model.proj_blend(ŷ_τ);                                             #@show size(h_) # (D_i, T, B)\n",
    "\n",
    "        ŷ_P = [i!=ttypeids[\"type\"] ? proj(h_) : ŷ_t_P\n",
    "                for (proj,i) in zip(model.projs, 1:length(model.projs))];       #@show size.(ŷ_P) # (N_T, T, B)|T=[1:7]    \n",
    "\n",
    "\n",
    "        if training\n",
    "                losses = [nll(ŷ_P[i].*reshape(loss_mask, (:, size(ŷ_P[i])[2:3]...)), y[:,i,:],\n",
    "                                average=false)[1]/sum(loss_mask)\n",
    "                                        for i in 1:length(n_ttypes)]\n",
    "                loss = mean(losses)\n",
    "                return loss\n",
    "        else\n",
    "                gen = Array{Int64}([\n",
    "                        sampling(ŷ_P[ttypeids[\"tempo\"]], t=1.2, p=0.9),\n",
    "                        sampling(ŷ_P[ttypeids[\"chord\"]], p=0.99),\n",
    "                        sampling(ŷ_P[ttypeids[\"bar-beat\"]], t=1.2),\n",
    "                        ŷ_t,\n",
    "                        sampling(ŷ_P[ttypeids[\"pitch\"]], p=0.9),\n",
    "                        sampling(ŷ_P[ttypeids[\"duration\"]], t=2, p=0.9),\n",
    "                        sampling(ŷ_P[ttypeids[\"velocity\"]], t=5)\n",
    "                ])\n",
    "                return gen, state\n",
    "        end\n",
    "end\n",
    "\n",
    "(model::CPTransformer)(x, y) = model(x, y=y)\n",
    "(model::CPTransformer)(d::Knet.Data) = mean(model(x, y) for (x, y) in d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knet AutoGrad for Cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Is this correct?\n",
    "@primitive cumsum(x; dims::Integer),dy,y  (cumsum(x.*dy, dims=dims)./y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_SMALL_MODEL = true\n",
    "\n",
    "# Model settings\n",
    "if RUN_SMALL_MODEL\n",
    "    # Small model for debugging\n",
    "    BATCH_SIZE = 4 ÷ 2\n",
    "    EMBED_SIZES = [128, 256, 64, 32, 512, 128, 128] .÷8\n",
    "    D_MODEL = 512 ÷16\n",
    "    D_INNER = 2048 ÷16\n",
    "    N_HEADS = 8 ÷4\n",
    "    N_LAYERS = 12 ÷6\n",
    "    DROPOUT = 0.1\n",
    "else\n",
    "    # Original model size\n",
    "    BATCH_SIZE = 4\n",
    "    EMBED_SIZES = [128, 256, 64, 32, 512, 128, 128]\n",
    "    D_MODEL = 512\n",
    "    D_INNER = 2048\n",
    "    N_HEADS = 8\n",
    "    N_LAYERS = 12\n",
    "    DROPOUT = 0.1\n",
    "end\n",
    "\n",
    "# Learning settings\n",
    "N_EPOCH = 5\n",
    "LR = 1e-4\n",
    "\n",
    "# GPU settings\n",
    "ATYPE = CuArray;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"..\\\\dataset\\\\representations\\\\uncond\\\\cp\\\\ailab17k_from-scratch_cp\"\n",
    "# dir = \"..\\\\..\\\\..\\\\..\\\\..\\\\datasets\\\\Pop1K7\\\\Pop1K7-CP\" # KUACC Dir\n",
    "\n",
    "t2i, i2t = Pickle.load(open(\"$(dir)\\\\dictionary.pkl\"))\n",
    "train = NPZ.npzread(\"$(dir)\\\\train_data_linear.npz\")\n",
    "test = NPZ.npzread(\"$(dir)\\\\test_data_linear.npz\")\n",
    "\n",
    "ttypeids = OrderedDict(\"tempo\"=>1,\"chord\"=>2,\"bar-beat\"=>3,\"type\"=>4,\"pitch\"=>5, \"duration\"=>6, \"velocity\"=>7) # token type ids\n",
    "n_ttypes = [length(t2i[k]) for (k, v) in ttypeids];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = trunc.(Int, permutedims(\n",
    "          cat(train[\"x\"], reshape(train[\"mask\"], (size(train[\"x\"],1),size(train[\"x\"],2),1)), dims=3),\n",
    "          [2, 3, 1]).+1);                                                                               # @show size(train_x) # T, K+1, B\n",
    "train_y = trunc.(Int, permutedims(train[\"y\"], [2, 3, 1]).+1);                                           # @show size(train_y) # T, K, B\n",
    "test_x = trunc.(Int, permutedims(\n",
    "         cat(test[\"x\"], reshape(test[\"mask\"], (size(test[\"x\"],1),size(test[\"x\"],2),1)), dims=3),\n",
    "         [2, 3, 1]).+1);                                                                                # @show size(test_x) # T, K+1, B\n",
    "test_y = trunc.(Int, permutedims(test[\"y\"], [2, 3, 1]).+1);                                             # @show size(test_y) # T, K, B\n",
    "\n",
    "train_loader = minibatch(train_x, train_y, BATCH_SIZE; shuffle=true)\n",
    "test_loader = minibatch(test_x, test_y, BATCH_SIZE; shuffle=true)\n",
    "\n",
    "length.((train_loader, test_loader));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 | Train Loss: 6.510648590010313 | Test Loss: 8.815053772495006\n",
      "Epoch #1 | Train Loss: 4.2602777305044555 | Test Loss: 7.112699467387899\n",
      "Epoch #2 | Train Loss: 4.090010221506058 | Test Loss: 6.478809779027545\n",
      "Epoch #3 | Train Loss: 4.010795622143956 | Test Loss: 6.466660133081239\n",
      "Epoch #4 | Train Loss: 4.014187392273251 | Test Loss: 6.346034141536454\n",
      "Epoch #5 | Train Loss: 3.9877309055502126 | Test Loss: 6.4471323984290665\n"
     ]
    }
   ],
   "source": [
    "Knet.seed!(121212)\n",
    "model = CPTransformer(n_ttypes, EMBED_SIZES, D_MODEL, D_INNER, N_HEADS, N_LAYERS, DROPOUT)\n",
    "train_losses = []; test_losses = []\n",
    "for epoch in 1:N_EPOCH+1\n",
    "    train_loss = model(train_loader); append!(train_losses, train_loss)\n",
    "    test_loss = model(test_loader); append!(test_losses, test_loss)\n",
    "    println(\"Epoch #$(epoch-1) | Train Loss: $(train_loss) | Test Loss: $(test_loss)\")\n",
    "    if epoch == N_EPOCH+1 break; end\n",
    "    if train_loss<=0.05 break; end\n",
    "    adam!(model, train_loader, lr=LR)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip120\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip120)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip121\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip120)\" d=\"\nM112.177 1486.45 L2352.76 1486.45 L2352.76 123.472 L112.177 123.472  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip122\">\n    <rect x=\"112\" y=\"123\" width=\"2242\" height=\"1364\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  175.59,1486.45 175.59,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  598.34,1486.45 598.34,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1021.09,1486.45 1021.09,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1443.84,1486.45 1443.84,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1866.59,1486.45 1866.59,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2289.34,1486.45 2289.34,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  175.59,1486.45 175.59,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  598.34,1486.45 598.34,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1021.09,1486.45 1021.09,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1443.84,1486.45 1443.84,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1866.59,1486.45 1866.59,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2289.34,1486.45 2289.34,1467.55 \n  \"/>\n<path clip-path=\"url(#clip120)\" d=\"M175.59 1517.37 Q171.979 1517.37 170.15 1520.93 Q168.344 1524.47 168.344 1531.6 Q168.344 1538.71 170.15 1542.27 Q171.979 1545.82 175.59 1545.82 Q179.224 1545.82 181.03 1542.27 Q182.858 1538.71 182.858 1531.6 Q182.858 1524.47 181.03 1520.93 Q179.224 1517.37 175.59 1517.37 M175.59 1513.66 Q181.4 1513.66 184.455 1518.27 Q187.534 1522.85 187.534 1531.6 Q187.534 1540.33 184.455 1544.94 Q181.4 1549.52 175.59 1549.52 Q169.78 1549.52 166.701 1544.94 Q163.645 1540.33 163.645 1531.6 Q163.645 1522.85 166.701 1518.27 Q169.78 1513.66 175.59 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M588.722 1544.91 L596.361 1544.91 L596.361 1518.55 L588.051 1520.21 L588.051 1515.95 L596.315 1514.29 L600.991 1514.29 L600.991 1544.91 L608.63 1544.91 L608.63 1548.85 L588.722 1548.85 L588.722 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M1015.74 1544.91 L1032.06 1544.91 L1032.06 1548.85 L1010.12 1548.85 L1010.12 1544.91 Q1012.78 1542.16 1017.36 1537.53 Q1021.97 1532.88 1023.15 1531.53 Q1025.4 1529.01 1026.28 1527.27 Q1027.18 1525.51 1027.18 1523.82 Q1027.18 1521.07 1025.23 1519.33 Q1023.31 1517.6 1020.21 1517.6 Q1018.01 1517.6 1015.56 1518.36 Q1013.13 1519.13 1010.35 1520.68 L1010.35 1515.95 Q1013.17 1514.82 1015.63 1514.24 Q1018.08 1513.66 1020.12 1513.66 Q1025.49 1513.66 1028.68 1516.35 Q1031.88 1519.03 1031.88 1523.52 Q1031.88 1525.65 1031.07 1527.57 Q1030.28 1529.47 1028.17 1532.07 Q1027.6 1532.74 1024.49 1535.95 Q1021.39 1539.15 1015.74 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M1448.09 1530.21 Q1451.45 1530.93 1453.32 1533.2 Q1455.22 1535.47 1455.22 1538.8 Q1455.22 1543.92 1451.7 1546.72 Q1448.18 1549.52 1441.7 1549.52 Q1439.52 1549.52 1437.21 1549.08 Q1434.92 1548.66 1432.46 1547.81 L1432.46 1543.29 Q1434.41 1544.43 1436.72 1545.01 Q1439.04 1545.58 1441.56 1545.58 Q1445.96 1545.58 1448.25 1543.85 Q1450.57 1542.11 1450.57 1538.8 Q1450.57 1535.75 1448.41 1534.03 Q1446.28 1532.3 1442.46 1532.3 L1438.44 1532.3 L1438.44 1528.45 L1442.65 1528.45 Q1446.1 1528.45 1447.93 1527.09 Q1449.76 1525.7 1449.76 1523.11 Q1449.76 1520.45 1447.86 1519.03 Q1445.98 1517.6 1442.46 1517.6 Q1440.54 1517.6 1438.34 1518.01 Q1436.15 1518.43 1433.51 1519.31 L1433.51 1515.14 Q1436.17 1514.4 1438.48 1514.03 Q1440.82 1513.66 1442.88 1513.66 Q1448.21 1513.66 1451.31 1516.09 Q1454.41 1518.5 1454.41 1522.62 Q1454.41 1525.49 1452.77 1527.48 Q1451.12 1529.45 1448.09 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M1869.6 1518.36 L1857.8 1536.81 L1869.6 1536.81 L1869.6 1518.36 M1868.37 1514.29 L1874.25 1514.29 L1874.25 1536.81 L1879.19 1536.81 L1879.19 1540.7 L1874.25 1540.7 L1874.25 1548.85 L1869.6 1548.85 L1869.6 1540.7 L1854 1540.7 L1854 1536.19 L1868.37 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M2279.62 1514.29 L2297.98 1514.29 L2297.98 1518.22 L2283.9 1518.22 L2283.9 1526.7 Q2284.92 1526.35 2285.94 1526.19 Q2286.96 1526 2287.98 1526 Q2293.76 1526 2297.14 1529.17 Q2300.52 1532.34 2300.52 1537.76 Q2300.52 1543.34 2297.05 1546.44 Q2293.58 1549.52 2287.26 1549.52 Q2285.08 1549.52 2282.82 1549.15 Q2280.57 1548.78 2278.16 1548.04 L2278.16 1543.34 Q2280.25 1544.47 2282.47 1545.03 Q2284.69 1545.58 2287.17 1545.58 Q2291.17 1545.58 2293.51 1543.48 Q2295.85 1541.37 2295.85 1537.76 Q2295.85 1534.15 2293.51 1532.04 Q2291.17 1529.94 2287.17 1529.94 Q2285.29 1529.94 2283.42 1530.35 Q2281.57 1530.77 2279.62 1531.65 L2279.62 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,1460.69 2352.76,1460.69 \n  \"/>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,1196.65 2352.76,1196.65 \n  \"/>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,932.61 2352.76,932.61 \n  \"/>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,668.569 2352.76,668.569 \n  \"/>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,404.529 2352.76,404.529 \n  \"/>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,140.488 2352.76,140.488 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,1486.45 112.177,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,1460.69 131.075,1460.69 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,1196.65 131.075,1196.65 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,932.61 131.075,932.61 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,668.569 131.075,668.569 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,404.529 131.075,404.529 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,140.488 131.075,140.488 \n  \"/>\n<path clip-path=\"url(#clip120)\" d=\"M66.5939 1447.49 L54.7884 1465.93 L66.5939 1465.93 L66.5939 1447.49 M65.367 1443.41 L71.2466 1443.41 L71.2466 1465.93 L76.1772 1465.93 L76.1772 1469.82 L71.2466 1469.82 L71.2466 1477.97 L66.5939 1477.97 L66.5939 1469.82 L50.9921 1469.82 L50.9921 1465.31 L65.367 1443.41 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M55.2745 1179.37 L73.6309 1179.37 L73.6309 1183.31 L59.5569 1183.31 L59.5569 1191.78 Q60.5754 1191.43 61.5939 1191.27 Q62.6124 1191.08 63.6309 1191.08 Q69.418 1191.08 72.7976 1194.25 Q76.1772 1197.43 76.1772 1202.84 Q76.1772 1208.42 72.705 1211.52 Q69.2328 1214.6 62.9134 1214.6 Q60.7374 1214.6 58.4689 1214.23 Q56.2236 1213.86 53.8162 1213.12 L53.8162 1208.42 Q55.8995 1209.56 58.1217 1210.11 Q60.3439 1210.67 62.8208 1210.67 Q66.8254 1210.67 69.1633 1208.56 Q71.5013 1206.45 71.5013 1202.84 Q71.5013 1199.23 69.1633 1197.13 Q66.8254 1195.02 62.8208 1195.02 Q60.9458 1195.02 59.0708 1195.44 Q57.2189 1195.85 55.2745 1196.73 L55.2745 1179.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M64.6495 930.747 Q61.5013 930.747 59.6495 932.899 Q57.8208 935.052 57.8208 938.802 Q57.8208 942.529 59.6495 944.705 Q61.5013 946.858 64.6495 946.858 Q67.7976 946.858 69.6263 944.705 Q71.4781 942.529 71.4781 938.802 Q71.4781 935.052 69.6263 932.899 Q67.7976 930.747 64.6495 930.747 M73.9318 916.094 L73.9318 920.353 Q72.1726 919.52 70.367 919.08 Q68.5846 918.64 66.8254 918.64 Q62.1958 918.64 59.7421 921.765 Q57.3115 924.89 56.9643 931.21 Q58.33 929.196 60.3902 928.131 Q62.4504 927.043 64.9272 927.043 Q70.1355 927.043 73.1448 930.214 Q76.1772 933.362 76.1772 938.802 Q76.1772 944.126 73.029 947.344 Q69.8809 950.561 64.6495 950.561 Q58.6541 950.561 55.4828 945.978 Q52.3116 941.372 52.3116 932.645 Q52.3116 924.45 56.2004 919.589 Q60.0893 914.705 66.6402 914.705 Q68.3994 914.705 70.1818 915.052 Q71.9874 915.4 73.9318 916.094 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M53.9551 651.289 L76.1772 651.289 L76.1772 653.28 L63.6309 685.849 L58.7467 685.849 L70.5522 655.224 L53.9551 655.224 L53.9551 651.289 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M64.3254 405.397 Q60.9921 405.397 59.0708 407.179 Q57.1726 408.961 57.1726 412.086 Q57.1726 415.211 59.0708 416.994 Q60.9921 418.776 64.3254 418.776 Q67.6587 418.776 69.58 416.994 Q71.5013 415.188 71.5013 412.086 Q71.5013 408.961 69.58 407.179 Q67.6819 405.397 64.3254 405.397 M59.6495 403.406 Q56.6402 402.665 54.9504 400.605 Q53.2838 398.545 53.2838 395.582 Q53.2838 391.438 56.2236 389.031 Q59.1865 386.624 64.3254 386.624 Q69.4874 386.624 72.4272 389.031 Q75.367 391.438 75.367 395.582 Q75.367 398.545 73.6772 400.605 Q72.0105 402.665 69.0244 403.406 Q72.404 404.193 74.279 406.485 Q76.1772 408.776 76.1772 412.086 Q76.1772 417.11 73.0985 419.795 Q70.0429 422.48 64.3254 422.48 Q58.6078 422.48 55.5291 419.795 Q52.4736 417.11 52.4736 412.086 Q52.4736 408.776 54.3717 406.485 Q56.2699 404.193 59.6495 403.406 M57.9365 396.022 Q57.9365 398.707 59.6032 400.211 Q61.293 401.716 64.3254 401.716 Q67.3346 401.716 69.0244 400.211 Q70.7374 398.707 70.7374 396.022 Q70.7374 393.337 69.0244 391.832 Q67.3346 390.327 64.3254 390.327 Q61.293 390.327 59.6032 391.832 Q57.9365 393.337 57.9365 396.022 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M54.5569 157.05 L54.5569 152.791 Q56.3162 153.624 58.1217 154.064 Q59.9273 154.504 61.6634 154.504 Q66.293 154.504 68.7235 151.402 Q71.1772 148.277 71.5244 141.935 Q70.1818 143.925 68.1217 144.99 Q66.0615 146.055 63.5615 146.055 Q58.3763 146.055 55.344 142.93 Q52.3347 139.782 52.3347 134.342 Q52.3347 129.018 55.4828 125.8 Q58.631 122.583 63.8624 122.583 Q69.8578 122.583 73.0059 127.189 Q76.1772 131.773 76.1772 140.523 Q76.1772 148.694 72.2883 153.578 Q68.4226 158.439 61.8717 158.439 Q60.1124 158.439 58.3069 158.092 Q56.5014 157.745 54.5569 157.05 M63.8624 142.398 Q67.0106 142.398 68.8393 140.245 Q70.6911 138.092 70.6911 134.342 Q70.6911 130.615 68.8393 128.462 Q67.0106 126.287 63.8624 126.287 Q60.7143 126.287 58.8625 128.462 Q57.0338 130.615 57.0338 134.342 Q57.0338 138.092 58.8625 140.245 Q60.7143 142.398 63.8624 142.398 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M1147.58 12.096 L1155.76 12.096 L1155.76 65.6895 L1185.21 65.6895 L1185.21 72.576 L1147.58 72.576 L1147.58 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M1209.6 32.4315 Q1203.6 32.4315 1200.12 37.1306 Q1196.64 41.7891 1196.64 49.9314 Q1196.64 58.0738 1200.08 62.7728 Q1203.56 67.4314 1209.6 67.4314 Q1215.55 67.4314 1219.04 62.7323 Q1222.52 58.0333 1222.52 49.9314 Q1222.52 41.8701 1219.04 37.1711 Q1215.55 32.4315 1209.6 32.4315 M1209.6 26.1121 Q1219.32 26.1121 1224.87 32.4315 Q1230.42 38.7509 1230.42 49.9314 Q1230.42 61.0714 1224.87 67.4314 Q1219.32 73.7508 1209.6 73.7508 Q1199.84 73.7508 1194.29 67.4314 Q1188.78 61.0714 1188.78 49.9314 Q1188.78 38.7509 1194.29 32.4315 Q1199.84 26.1121 1209.6 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M1271.7 28.5427 L1271.7 35.5912 Q1268.54 33.9709 1265.14 33.1607 Q1261.73 32.3505 1258.09 32.3505 Q1252.54 32.3505 1249.74 34.0519 Q1246.99 35.7533 1246.99 39.156 Q1246.99 41.7486 1248.97 43.2475 Q1250.96 44.7058 1256.95 46.0426 L1259.51 46.6097 Q1267.45 48.3111 1270.77 51.4303 Q1274.13 54.509 1274.13 60.0587 Q1274.13 66.3781 1269.11 70.0644 Q1264.12 73.7508 1255.37 73.7508 Q1251.73 73.7508 1247.76 73.0216 Q1243.83 72.3329 1239.45 70.9151 L1239.45 63.2184 Q1243.59 65.3654 1247.6 66.4591 Q1251.61 67.5124 1255.54 67.5124 Q1260.8 67.5124 1263.64 65.73 Q1266.47 63.9071 1266.47 60.6258 Q1266.47 57.5877 1264.41 55.9673 Q1262.38 54.3469 1255.46 52.8481 L1252.86 52.2405 Q1245.94 50.7821 1242.86 47.7845 Q1239.78 44.7463 1239.78 39.4801 Q1239.78 33.0797 1244.32 29.5959 Q1248.85 26.1121 1257.2 26.1121 Q1261.33 26.1121 1264.98 26.7198 Q1268.62 27.3274 1271.7 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M1314.92 28.5427 L1314.92 35.5912 Q1311.76 33.9709 1308.36 33.1607 Q1304.96 32.3505 1301.31 32.3505 Q1295.76 32.3505 1292.97 34.0519 Q1290.21 35.7533 1290.21 39.156 Q1290.21 41.7486 1292.2 43.2475 Q1294.18 44.7058 1300.18 46.0426 L1302.73 46.6097 Q1310.67 48.3111 1313.99 51.4303 Q1317.35 54.509 1317.35 60.0587 Q1317.35 66.3781 1312.33 70.0644 Q1307.35 73.7508 1298.6 73.7508 Q1294.95 73.7508 1290.98 73.0216 Q1287.05 72.3329 1282.68 70.9151 L1282.68 63.2184 Q1286.81 65.3654 1290.82 66.4591 Q1294.83 67.5124 1298.76 67.5124 Q1304.03 67.5124 1306.86 65.73 Q1309.7 63.9071 1309.7 60.6258 Q1309.7 57.5877 1307.63 55.9673 Q1305.61 54.3469 1298.68 52.8481 L1296.09 52.2405 Q1289.16 50.7821 1286.08 47.7845 Q1283 44.7463 1283 39.4801 Q1283 33.0797 1287.54 29.5959 Q1292.08 26.1121 1300.42 26.1121 Q1304.55 26.1121 1308.2 26.7198 Q1311.84 27.3274 1314.92 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip122)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  175.59,162.047 598.34,755.075 1021.09,779.124 1443.84,823.851 1866.59,666.118 2289.34,841.075 \n  \"/>\n<polyline clip-path=\"url(#clip122)\" style=\"stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  175.59,829.393 598.34,1378.34 1021.09,1430.83 1443.84,1433.12 1866.59,1447.24 2289.34,1447.87 \n  \"/>\n<path clip-path=\"url(#clip120)\" d=\"\nM1919.26 324.425 L2278.07 324.425 L2278.07 168.905 L1919.26 168.905  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1919.26,324.425 2278.07,324.425 2278.07,168.905 1919.26,168.905 1919.26,324.425 \n  \"/>\n<polyline clip-path=\"url(#clip120)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1944.15,220.745 2093.52,220.745 \n  \"/>\n<path clip-path=\"url(#clip120)\" d=\"M2118.42 203.465 L2147.65 203.465 L2147.65 207.4 L2135.39 207.4 L2135.39 238.025 L2130.69 238.025 L2130.69 207.4 L2118.42 207.4 L2118.42 203.465 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M2166.1 223.997 L2166.1 226.08 L2146.52 226.08 Q2146.8 230.478 2149.16 232.793 Q2151.54 235.085 2155.78 235.085 Q2158.23 235.085 2160.53 234.483 Q2162.84 233.881 2165.11 232.677 L2165.11 236.705 Q2162.82 237.677 2160.41 238.187 Q2158 238.696 2155.53 238.696 Q2149.32 238.696 2145.69 235.085 Q2142.08 231.474 2142.08 225.316 Q2142.08 218.951 2145.5 215.224 Q2148.95 211.474 2154.78 211.474 Q2160.02 211.474 2163.05 214.853 Q2166.1 218.21 2166.1 223.997 M2161.84 222.747 Q2161.8 219.252 2159.88 217.168 Q2157.98 215.085 2154.83 215.085 Q2151.27 215.085 2149.11 217.099 Q2146.98 219.113 2146.66 222.77 L2161.84 222.747 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M2189.62 212.863 L2189.62 216.891 Q2187.82 215.965 2185.87 215.502 Q2183.93 215.039 2181.84 215.039 Q2178.67 215.039 2177.08 216.011 Q2175.5 216.983 2175.5 218.928 Q2175.5 220.409 2176.64 221.265 Q2177.77 222.099 2181.2 222.863 L2182.65 223.187 Q2187.19 224.159 2189.09 225.941 Q2191.01 227.701 2191.01 230.872 Q2191.01 234.483 2188.14 236.589 Q2185.29 238.696 2180.29 238.696 Q2178.21 238.696 2175.94 238.279 Q2173.7 237.886 2171.2 237.076 L2171.2 232.677 Q2173.56 233.904 2175.85 234.529 Q2178.14 235.131 2180.39 235.131 Q2183.4 235.131 2185.02 234.113 Q2186.64 233.071 2186.64 231.196 Q2186.64 229.46 2185.46 228.534 Q2184.3 227.608 2180.34 226.752 L2178.86 226.404 Q2174.9 225.571 2173.14 223.858 Q2171.38 222.122 2171.38 219.113 Q2171.38 215.455 2173.97 213.465 Q2176.57 211.474 2181.34 211.474 Q2183.7 211.474 2185.78 211.821 Q2187.86 212.168 2189.62 212.863 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M2202.01 204.738 L2202.01 212.099 L2210.78 212.099 L2210.78 215.409 L2202.01 215.409 L2202.01 229.483 Q2202.01 232.654 2202.86 233.557 Q2203.74 234.46 2206.4 234.46 L2210.78 234.46 L2210.78 238.025 L2206.4 238.025 Q2201.47 238.025 2199.6 236.196 Q2197.72 234.344 2197.72 229.483 L2197.72 215.409 L2194.6 215.409 L2194.6 212.099 L2197.72 212.099 L2197.72 204.738 L2202.01 204.738 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip120)\" style=\"stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1944.15,272.585 2093.52,272.585 \n  \"/>\n<path clip-path=\"url(#clip120)\" d=\"M2118.42 255.305 L2147.65 255.305 L2147.65 259.24 L2135.39 259.24 L2135.39 289.865 L2130.69 289.865 L2130.69 259.24 L2118.42 259.24 L2118.42 255.305 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M2160.04 267.92 Q2159.32 267.504 2158.46 267.318 Q2157.63 267.11 2156.61 267.11 Q2153 267.11 2151.06 269.471 Q2149.14 271.809 2149.14 276.207 L2149.14 289.865 L2144.85 289.865 L2144.85 263.939 L2149.14 263.939 L2149.14 267.967 Q2150.48 265.606 2152.63 264.471 Q2154.78 263.314 2157.86 263.314 Q2158.3 263.314 2158.84 263.383 Q2159.37 263.43 2160.02 263.545 L2160.04 267.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M2176.29 276.832 Q2171.13 276.832 2169.14 278.013 Q2167.15 279.193 2167.15 282.041 Q2167.15 284.309 2168.63 285.652 Q2170.13 286.971 2172.7 286.971 Q2176.24 286.971 2178.37 284.471 Q2180.52 281.948 2180.52 277.781 L2180.52 276.832 L2176.29 276.832 M2184.78 275.073 L2184.78 289.865 L2180.52 289.865 L2180.52 285.929 Q2179.07 288.291 2176.89 289.425 Q2174.71 290.536 2171.57 290.536 Q2167.59 290.536 2165.22 288.314 Q2162.89 286.068 2162.89 282.318 Q2162.89 277.943 2165.8 275.721 Q2168.74 273.499 2174.55 273.499 L2180.52 273.499 L2180.52 273.082 Q2180.52 270.143 2178.58 268.545 Q2176.66 266.925 2173.16 266.925 Q2170.94 266.925 2168.84 267.457 Q2166.73 267.99 2164.78 269.055 L2164.78 265.119 Q2167.12 264.217 2169.32 263.777 Q2171.52 263.314 2173.6 263.314 Q2179.23 263.314 2182.01 266.231 Q2184.78 269.147 2184.78 275.073 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M2193.56 263.939 L2197.82 263.939 L2197.82 289.865 L2193.56 289.865 L2193.56 263.939 M2193.56 253.846 L2197.82 253.846 L2197.82 259.24 L2193.56 259.24 L2193.56 253.846 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip120)\" d=\"M2228.28 274.217 L2228.28 289.865 L2224.02 289.865 L2224.02 274.355 Q2224.02 270.675 2222.58 268.846 Q2221.15 267.018 2218.28 267.018 Q2214.83 267.018 2212.84 269.217 Q2210.85 271.416 2210.85 275.212 L2210.85 289.865 L2206.57 289.865 L2206.57 263.939 L2210.85 263.939 L2210.85 267.967 Q2212.38 265.629 2214.44 264.471 Q2216.52 263.314 2219.23 263.314 Q2223.7 263.314 2225.99 266.092 Q2228.28 268.846 2228.28 274.217 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results | Train Loss: 4.048547239504521 | Test Loss: 6.34667094825166\n"
     ]
    }
   ],
   "source": [
    "println(\"Final Results | Train Loss: $(train_losses[end]) | Test Loss: $(test_losses[end])\")\n",
    "loss_plot = plot(0:length(test_losses)-1, [test_losses, train_losses], labels=[\"Test\" \"Train\"], title=\"Loss\")\n",
    "display(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000×7 Matrix{Int64}:\n",
       "  0   0   0  2  70  11  10\n",
       "  1   0   6  1   0   0   7\n",
       "  0   0   0  2  51  15   1\n",
       "  1  45  12  1   0   0  11\n",
       "  0   0   0  2  43  14  15\n",
       "  1  45   6  1   0   0   0\n",
       "  0   0   0  2  35  15   6\n",
       " 16  45   6  1   0   0   2\n",
       "  0   0   0  2  60   5  16\n",
       "  0  45   1  1   0   0   0\n",
       "  0   0   0  2  61  13  18\n",
       "  0   0   0  2  45  13   6\n",
       "  0   0   0  2  53  14  15\n",
       "  ⋮                  ⋮  \n",
       "  8  45   3  1   0   0  16\n",
       "  0   0   0  2  68   3   2\n",
       "  0  45  16  1   0  14  10\n",
       "  0   0   0  2  68   3   7\n",
       "  0   0   3  2  54  17   4\n",
       "  0   0   0  2  49   4   9\n",
       "  0  45  15  1   0   0   0\n",
       "  0   0   0  2  64  10   8\n",
       "  1  45   6  1   0   0  20\n",
       "  0   0   0  2  66  14  20\n",
       "  0   0   0  2  46   7  18\n",
       "  0   0   0  2  59  14   8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ATYPE([1,1,2,2,1,1,1])\n",
    "state = nothing\n",
    "composition = []\n",
    "bars = 1\n",
    "while length(composition) < 5000\n",
    "    x, state = model(reshape(x, (1,7,1)), state=state)\n",
    "    append!(composition, [reshape(x.-1, (1, 7))])\n",
    "    if i2t[\"type\"][x[4]-1] == \"EOS\" break; end\n",
    "    if i2t[\"bar-beat\"][x[3]-1] == \"Bar\" bars+=1; end\n",
    "end\n",
    "# append!(composition, [reshape([0,0,0,0,0,0,0], (1, 7))])\n",
    "composition = vcat(composition...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CP in NP format.\n",
    "dir = \"out\"\n",
    "NPZ.npzwrite(\"$(dir)\\\\cp\\\\comp2.npz\", composition)\n",
    "\n",
    "# PyCall\n",
    "# using PyCall\n",
    "# pushfirst!(PyVector(pyimport(\"sys\").\"path\"), \"\")\n",
    "# write_midi = pyimport(\"write_midi\")\n",
    "# Load midi function.\n",
    "# Load CP in NP format.\n",
    "# Load word2event dictionary.\n",
    "# Call function (composition, src, word2event)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "07d7f1db1109769b3e76de940780c60d78e4999d8c8368b5454c1eb47431d439"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
