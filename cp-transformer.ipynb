{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mOMEinsum loaded the CUDA module successfully\n"
     ]
    }
   ],
   "source": [
    "using Knet#: Knet, minibatch, param, param0, softmax, nll, bmm, relu, dropout, progress!, adam\n",
    "using Knet: AutoGrad.@primitive\n",
    "using CUDA: CuArray\n",
    "using Pickle\n",
    "using NPZ\n",
    "using Statistics: mean, std\n",
    "using StatsBase: sample, Weights\n",
    "using DataStructures: OrderedDict\n",
    "using LinearAlgebra\n",
    "using OMEinsum\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### Base Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Linear; W; b; end\n",
    "Linear(i::Int, o...) =\n",
    "    Linear(param(o..., i, atype=ATYPE), param0(o..., atype=ATYPE))\n",
    "(l::Linear)(x) =\n",
    "    reshape(reshape(l.W,:,size(l.W)[end]) * reshape(x,size(x)[1],:), size(l.W)[1:end-1]..., size(x)[2:end]...) .+ l.b\n",
    "\n",
    "struct Activation; f; o; end\n",
    "Activation(f, o...) = \n",
    "    Activation(f, o)\n",
    "(a::Activation)(x) =\n",
    "    a.f.(x, a.o...)\n",
    "\n",
    "struct Embedding; W; end\n",
    "Embedding(n_ttypes::Int, embed::Int) =\n",
    "    Embedding(param(embed, n_ttypes, atype=ATYPE))\n",
    "(e::Embedding)(x) = \n",
    "    e.W[:, x] * √size(e.W, 1)\n",
    "\n",
    "struct Dropout; p; end\n",
    "(d::Dropout)(x) =\n",
    "    dropout(x, d.p)\n",
    "\n",
    "struct LayerNorm; W; b; ϵ; end\n",
    "LayerNorm(d_model; eps=1e-6) = \n",
    "    LayerNorm(param(d_model; init=ones, atype=ATYPE),\n",
    "              param(d_model, init=zeros, atype=ATYPE),\n",
    "              eps)\n",
    "function (ln::LayerNorm)(x, o...; dims=1)\n",
    "    μ = mean(x, dims=dims)\n",
    "    σ = std(x, mean=μ, dims=dims, corrected=false)\n",
    "    ln.W .* (x .- μ) ./ (σ .+ eltype(x)(ln.ϵ)) .+ ln.b\n",
    "end\n",
    "\n",
    "struct PositionalEncoding; W; dropout; end\n",
    "function PositionalEncoding(d_model::Int; dropout=0.1, max_len=20000)\n",
    "    W = zeros(d_model, max_len)\n",
    "    pos = exp.((0:2:d_model-1) .* (-log(10000)/d_model)) * (0:max_len-1)'\n",
    "    W[1:2:end, :] = sin.(pos)\n",
    "    W[2:2:end, :] = cos.(pos)\n",
    "    PositionalEncoding(ATYPE(W), Dropout(dropout))\n",
    "end\n",
    "function (pe::PositionalEncoding)(x)\n",
    "    pe.dropout(x .+ pe.W[:, 1:size(x,2), :])\n",
    "end\n",
    "\n",
    "struct Chain; layers; end\n",
    "Chain(l, l_...) =\n",
    "    Chain(append!(Vector{Any}([l]), l_))\n",
    "function (c::Chain)(x, o...; state=nothing)\n",
    "    training = state == nothing\n",
    "    for layer in c.layers\n",
    "        if (typeof(layer) != TransformerEncoder) || training\n",
    "            x = layer(x, o...)\n",
    "        else\n",
    "            x, state = layer(x, o..., state=state)\n",
    "        end\n",
    "    end\n",
    "    training ? x : (x, state)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/2006.16236.pdf\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/attention_layer.py\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/recurrent/attention/self_attention/attention_layer.py\n",
    "struct Attention; q; k; v; o; inner; masker; end\n",
    "Attention(d_model::Int, n_heads::Int, inner, masker) =\n",
    "    Attention(Linear(d_model, d_model÷n_heads, n_heads),\n",
    "              Linear(d_model, d_model÷n_heads, n_heads),\n",
    "              Linear(d_model, d_model÷n_heads, n_heads),\n",
    "              Linear(d_model, d_model),\n",
    "              inner, masker)\n",
    "function (a::Attention)(q, k, v; state=nothing)\n",
    "    training = state==nothing\n",
    "    if training\n",
    "        _, T, B = size(q); H = size(a.q.W, 2)\n",
    "\n",
    "        q = reshape(a.q(q), (T, :, H, B))\n",
    "        k = reshape(a.k(k), (T, :, H, B))\n",
    "        v = reshape(a.v(v), (T, :, H, B))\n",
    "\n",
    "        o = a.inner(q, k, v, mask=a.masker(T))\n",
    "        return a.o(reshape(o, (:, T, B)))\n",
    "    else\n",
    "        _, _, B = size(q); H = size(a.q.W, 2)\n",
    "\n",
    "        q = reshape(a.q(q), (B, H, :))\n",
    "        k = reshape(a.k(k), (B, H, :))\n",
    "        v = reshape(a.v(v), (B, H, :))\n",
    "    \n",
    "        v_, state = a.inner(q, k, v, state=state)\n",
    "        o = a.o(reshape(v_, (:, B)))\n",
    "        return o, state\n",
    "    end\n",
    "end\n",
    "(a::Attention)(x; state=nothing) = a(x, x, x, state=state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Linear Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lower_triangular_mask (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://arxiv.org/pdf/2006.16236.pdf\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/causal_linear_attention.py\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/recurrent/attention/self_attention/linear_attention.py\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/causal_product/causal_product_cpu.cpp\n",
    "function causal_linear_attention(q, k, v; ϕ=gelu, ϵ=1e-6, mask=nothing, state=nothing)\n",
    "    training = state==nothing\n",
    "    if training\n",
    "        Q = ϕ.(q)\n",
    "        K = ϕ.(k)\n",
    "        K_ = permutedims(cumsum(permutedims(K, [2,1,3,4]), dims=2), [2,1,3,4])\n",
    "        Z = 1 ./ (sum(Q .* K_, dims=2) .+ ϵ)\n",
    "        S = bmm(K, v, transB=true)\n",
    "        S = S .* mask\n",
    "        QS = bmm(Q, S, transA=true)\n",
    "        V = permutedims(QS, [2,1,3,4])\n",
    "        return V .* Z\n",
    "    else\n",
    "        Q = ϕ.(q)\n",
    "        K = ϕ.(k)\n",
    "        if state==\"init\"\n",
    "            Si = ATYPE(zeros(size(Q)..., size(v,3)))\n",
    "            Zi = ATYPE(zeros(size(Q)...))\n",
    "        else\n",
    "            Si, Zi = state\n",
    "        end\n",
    "        @assert size(Si, 1) == size(Q, 1)\n",
    "        Zi = Zi .+ K\n",
    "        Si = Si .+ ein\"bhd, bhm -> bhdm\"(K, v)\n",
    "        Z = 1 ./ (ein\"bhd, bhd -> bh\"(Q, Zi) .+ ϵ)\n",
    "        V = ein\"bhd, bhdm, bh -> bhm\"(Q, Si, Z)\n",
    "        return V, (Si, Zi)\n",
    "    end\n",
    "end\n",
    "\n",
    "gelu(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x^3)));\n",
    "lower_triangular_mask(N) = ATYPE(Matrix{Float32}(UpperTriangular(ones(N,N))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/2006.16236.pdf\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/transformers.py\n",
    "# https://github.com/idiap/fast-transformers/blob/master/fast_transformers/recurrent/transformers.py\n",
    "struct TransformerEncoder; attention; drop; norm1; fc; norm2; end\n",
    "function TransformerEncoder(d_model::Int, d_ff::Int, n_heads::Int, dropout, activation, aparams...)\n",
    "    TransformerEncoder(\n",
    "        Attention(d_model, n_heads, causal_linear_attention, lower_triangular_mask),\n",
    "        Dropout(dropout),\n",
    "        LayerNorm(d_model),\n",
    "        Chain(Linear(d_model, d_ff), Activation(activation, aparams...), Dropout(dropout),\n",
    "            Linear(d_ff, d_model), Dropout(dropout)),\n",
    "        LayerNorm(d_model))\n",
    "end\n",
    "function (enc::TransformerEncoder)(x; state=nothing)\n",
    "    training = state == nothing\n",
    "    if training\n",
    "        x_ = enc.attention(x)\n",
    "        x = x .+ enc.drop(x_)\n",
    "        y = x = enc.norm1(x)\n",
    "        y = enc.fc(y)\n",
    "        return enc.norm2(x+y)\n",
    "    else\n",
    "        x_, state = enc.attention(x, state=state)\n",
    "        x = x .+ enc.drop(x_)\n",
    "        y = x = enc.norm1(x)\n",
    "        y = enc.fc(y)\n",
    "        return (enc.norm2(x+y), state)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nucleus (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sampling(x; t=1.0, p=nothing)\n",
    "    x = Vector{Float64}(reshape(x, length(x)))\n",
    "    x = softmax(x./t)\n",
    "    x = p!=nothing ? nucleus(x, p) : x\n",
    "    c = sample(1:length(x), Weights(x))\n",
    "    return c\n",
    "end\n",
    "\n",
    "function nucleus(x, p; ϵ=1e-5)\n",
    "    x = x./(sum(x)+ϵ)\n",
    "    i = sortperm(x, rev=true)\n",
    "    x_ = cumsum(sort(x, rev=true)).>p\n",
    "    c = sum(x_)>0 ? i[1:findfirst(x_)] : i\n",
    "    x = [i in c ? x[i] : 0 for i in 1:length(x)]\n",
    "    x = x./sum(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound Word Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CPTransformer; embeds; fc_in; posenc; enc; projs; proj_blend; end\n",
    "\n",
    "CPTransformer(n_ttypes::Vector{Int}, embed_sizes::Vector{Int}, d_model::Int, d_inner::Int, n_heads::Int, n_layers::Int, dropout) =\n",
    "    CPTransformer([Embedding(n, e) for (n, e) in zip(n_ttypes, embed_sizes)],\n",
    "            Linear(sum(embed_sizes), d_model),\n",
    "            PositionalEncoding(d_model),\n",
    "            Chain([TransformerEncoder(d_model, d_inner, n_heads, dropout, relu) for n=1:n_layers]..., LayerNorm(d_model)),\n",
    "            [Linear(d_model, n) for n in n_ttypes],\n",
    "            Linear(d_model + embed_sizes[ttypeids[\"type\"]], d_model))\n",
    "\n",
    "function (model::CPTransformer)(x; y=nothing, state=nothing)\n",
    "        training = y!=nothing\n",
    "\n",
    "        if training\n",
    "                x, loss_mask = x[:, 1:end-1, :], ATYPE(x[:, end, :].-1);        #@show size(x) # (T, N, B)\n",
    "        elseif state==nothing\n",
    "                state=\"init\"\n",
    "        end\n",
    "\n",
    "        x = vcat([embed(x[:, i, :]) for (embed, i) in\n",
    "                zip(model.embeds, 1:length(model.embeds))]...);                 #@show size(x) # (D_e, T, B)\n",
    "        \n",
    "        x = model.fc_in(x);                                                     #@show size(x) # (D_i, T, B)\n",
    "\n",
    "        x = model.posenc(x);                                                    #@show size(x) # (D_i, T, B)\n",
    "\n",
    "        if training \n",
    "                h = model.enc(x)\n",
    "        else \n",
    "                h, state = model.enc(x, state=state)\n",
    "        end;                                                                    #@show size(h) # (D_i, T, B)\n",
    "\n",
    "        ŷ_t_P = model.projs[ttypeids[\"type\"]](h);                               #@show size(ŷ_t_P) # (N_t, T, B)\n",
    "\n",
    "        ŷ_t = training ?\n",
    "                y[:, ttypeids[\"type\"], :] : \n",
    "                sampling(ŷ_t_P, p=0.9);                                         #@show size(ŷ_t) # (T, B)\n",
    "                \n",
    "        ŷ_τ = vcat([h, model.embeds[ttypeids[\"type\"]](ŷ_t)]...);                #@show size(ŷ_τ) # (D_i + D_t, T, B)\n",
    "\n",
    "        h_ = model.proj_blend(ŷ_τ);                                             #@show size(h_) # (D_i, T, B)\n",
    "\n",
    "        ŷ_P = [i!=ttypeids[\"type\"] ? proj(h_) : ŷ_t_P\n",
    "                for (proj,i) in zip(model.projs, 1:length(model.projs))];       #@show size.(ŷ_P) # (N_T, T, B)|T=[1:7]    \n",
    "\n",
    "\n",
    "        if training\n",
    "                losses = [nll(ŷ_P[i].*reshape(loss_mask, (:, size(ŷ_P[i])[2:3]...)), y[:,i,:],\n",
    "                                average=false)[1]/sum(loss_mask)\n",
    "                                        for i in 1:length(n_ttypes)]\n",
    "                loss = mean(losses)\n",
    "                return loss\n",
    "        else\n",
    "                gen = Array{Int64}([\n",
    "                        sampling(ŷ_P[ttypeids[\"tempo\"]], t=1.2, p=0.9),\n",
    "                        sampling(ŷ_P[ttypeids[\"chord\"]], p=0.99),\n",
    "                        sampling(ŷ_P[ttypeids[\"bar-beat\"]], t=1.2),\n",
    "                        ŷ_t,\n",
    "                        sampling(ŷ_P[ttypeids[\"pitch\"]], p=0.9),\n",
    "                        sampling(ŷ_P[ttypeids[\"duration\"]], t=2, p=0.9),\n",
    "                        sampling(ŷ_P[ttypeids[\"velocity\"]], t=5)\n",
    "                ])\n",
    "                return gen, state\n",
    "        end\n",
    "end\n",
    "\n",
    "(model::CPTransformer)(x, y) = model(x, y=y)\n",
    "(model::CPTransformer)(d::Knet.Data) = mean(model(x, y) for (x, y) in d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knet AutoGrad for Cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Is this correct?\n",
    "@primitive cumsum(x; dims::Integer),dy,y  (cumsum(x.*dy, dims=dims)./y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_SMALL_MODEL = false\n",
    "\n",
    "# Model settings\n",
    "if RUN_SMALL_MODEL\n",
    "    # Small model for debugging\n",
    "    BATCH_SIZE = 4 ÷ 2\n",
    "    EMBED_SIZES = [128, 256, 64, 32, 512, 128, 128] .÷8\n",
    "    D_MODEL = 512 ÷16\n",
    "    D_INNER = 2048 ÷16\n",
    "    N_HEADS = 8 ÷4\n",
    "    N_LAYERS = 12 ÷6\n",
    "    DROPOUT = 0.1\n",
    "else\n",
    "    # Original model size\n",
    "    BATCH_SIZE = 4 ÷2\n",
    "    EMBED_SIZES = [128, 256, 64, 32, 512, 128, 128]\n",
    "    D_MODEL = 512 ÷2\n",
    "    D_INNER = 2048 ÷2\n",
    "    N_HEADS = 8 ÷2\n",
    "    N_LAYERS = 12 ÷2\n",
    "    DROPOUT = 0.1\n",
    "end\n",
    "\n",
    "# Learning settings\n",
    "N_EPOCH = 20\n",
    "LR = 1e-4\n",
    "\n",
    "# GPU settings\n",
    "ATYPE = CuArray;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = \"..\\\\..\\\\..\\\\datasets\\\\Pop1K7\\\\Pop1K7-CP\"\n",
    "cd(homedir());cd(\"..\");cd(\"..\");cd(\"..\");cd(\"datasets\");cd(\"Pop1K7\");cd(\"Pop1K7-CP\")\n",
    "t2i, i2t = Pickle.load(open(\"dictionary.pkl\"))\n",
    "train = NPZ.npzread(\"train_data_linear.npz\")\n",
    "test = NPZ.npzread(\"test_data_linear.npz\")\n",
    "cd(homedir())\n",
    "\n",
    "ttypeids = OrderedDict(\"tempo\"=>1,\"chord\"=>2,\"bar-beat\"=>3,\"type\"=>4,\"pitch\"=>5, \"duration\"=>6, \"velocity\"=>7) # token type ids\n",
    "n_ttypes = [length(t2i[k]) for (k, v) in ttypeids];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = trunc.(Int, permutedims(\n",
    "          cat(train[\"x\"], reshape(train[\"mask\"], (size(train[\"x\"],1),size(train[\"x\"],2),1)), dims=3),\n",
    "          [2, 3, 1]).+1);                                                                               # @show size(train_x) # T, K+1, B\n",
    "train_y = trunc.(Int, permutedims(train[\"y\"], [2, 3, 1]).+1);                                           # @show size(train_y) # T, K, B\n",
    "test_x = trunc.(Int, permutedims(\n",
    "         cat(test[\"x\"], reshape(test[\"mask\"], (size(test[\"x\"],1),size(test[\"x\"],2),1)), dims=3),\n",
    "         [2, 3, 1]).+1);                                                                                # @show size(test_x) # T, K+1, B\n",
    "test_y = trunc.(Int, permutedims(test[\"y\"], [2, 3, 1]).+1);                                             # @show size(test_y) # T, K, B\n",
    "\n",
    "train_loader = minibatch(train_x, train_y, BATCH_SIZE; shuffle=true)\n",
    "test_loader = minibatch(test_x, test_y, BATCH_SIZE; shuffle=true)\n",
    "\n",
    "length.((train_loader, test_loader));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 | Train Loss: 7.003923977208512 | Test Loss: 9.6912366121113\n",
      "Epoch #1 | Train Loss: 4.058409500601285 | Test Loss: 6.612947665345753\n",
      "Epoch #2 | Train Loss: 4.0509957901940465 | Test Loss: 6.612565540474933\n",
      "Epoch #3 | Train Loss: 4.061101489872582 | Test Loss: 6.443135870736285\n",
      "Epoch #4 | Train Loss: 4.034590997950635 | Test Loss: 6.499217081785157\n",
      "Epoch #5 | Train Loss: 4.023097211101674 | Test Loss: 6.349373798914091\n",
      "Epoch #6 | Train Loss: 4.041295094016999 | Test Loss: 6.524848180596565\n",
      "Epoch #7 | Train Loss: 4.047148773975202 | Test Loss: 6.605444499228042\n",
      "Epoch #8 | Train Loss: 4.0515743880375625 | Test Loss: 6.393858751171974\n",
      "Epoch #9 | Train Loss: 4.038084723917413 | Test Loss: 6.593461417195264\n",
      "Epoch #10 | Train Loss: 4.039053119873245 | Test Loss: 6.641806643666159\n",
      "Epoch #11 | Train Loss: 4.060407888058052 | Test Loss: 6.388094245629831\n",
      "Epoch #12 | Train Loss: 4.045512229129938 | Test Loss: 6.568225444967714\n",
      "Epoch #13 | Train Loss: 4.0148259214313455 | Test Loss: 6.443286350520184\n",
      "Epoch #14 | Train Loss: 4.045730870718889 | Test Loss: 6.278816524974918\n",
      "Epoch #15 | Train Loss: 4.043678900025086 | Test Loss: 6.819511325360832\n",
      "Epoch #16 | Train Loss: 4.036106212440223 | Test Loss: 6.490480149646934\n",
      "Epoch #17 | Train Loss: 4.06436942861037 | Test Loss: 6.268437619232534\n",
      "Epoch #18 | Train Loss: 4.032028133260253 | Test Loss: 6.346791979082932\n",
      "Epoch #19 | Train Loss: 4.0557122259496 | Test Loss: 6.553572411646274\n",
      "Epoch #20 | Train Loss: 4.0299144580644 | Test Loss: 6.852181682815944\n"
     ]
    }
   ],
   "source": [
    "Knet.seed!(121212)\n",
    "model = CPTransformer(n_ttypes, EMBED_SIZES, D_MODEL, D_INNER, N_HEADS, N_LAYERS, DROPOUT)\n",
    "train_losses = []; test_losses = []\n",
    "for epoch in 1:N_EPOCH+1\n",
    "    train_loss = model(train_loader); append!(train_losses, train_loss)\n",
    "    test_loss = model(test_loader); append!(test_losses, test_loss)\n",
    "    println(\"Epoch #$(epoch-1) | Train Loss: $(train_loss) | Test Loss: $(test_loss)\")\n",
    "    if epoch == N_EPOCH+1 break; end\n",
    "    if train_loss<=0.05 break; end\n",
    "    adam!(model, train_loader, lr=LR)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results | Train Loss: 4.0299144580644 | Test Loss: 6.852181682815944\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip160\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip160)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip161\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip160)\" d=\"\nM112.177 1486.45 L2352.76 1486.45 L2352.76 123.472 L112.177 123.472  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip162\">\n    <rect x=\"112\" y=\"123\" width=\"2242\" height=\"1364\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip162)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  175.59,1486.45 175.59,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip162)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  704.028,1486.45 704.028,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip162)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1232.47,1486.45 1232.47,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip162)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1760.9,1486.45 1760.9,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip162)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2289.34,1486.45 2289.34,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  175.59,1486.45 175.59,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  704.028,1486.45 704.028,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1232.47,1486.45 1232.47,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1760.9,1486.45 1760.9,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2289.34,1486.45 2289.34,1467.55 \n  \"/>\n<path clip-path=\"url(#clip160)\" d=\"M175.59 1517.37 Q171.979 1517.37 170.15 1520.93 Q168.344 1524.47 168.344 1531.6 Q168.344 1538.71 170.15 1542.27 Q171.979 1545.82 175.59 1545.82 Q179.224 1545.82 181.03 1542.27 Q182.858 1538.71 182.858 1531.6 Q182.858 1524.47 181.03 1520.93 Q179.224 1517.37 175.59 1517.37 M175.59 1513.66 Q181.4 1513.66 184.455 1518.27 Q187.534 1522.85 187.534 1531.6 Q187.534 1540.33 184.455 1544.94 Q181.4 1549.52 175.59 1549.52 Q169.78 1549.52 166.701 1544.94 Q163.645 1540.33 163.645 1531.6 Q163.645 1522.85 166.701 1518.27 Q169.78 1513.66 175.59 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M694.306 1514.29 L712.662 1514.29 L712.662 1518.22 L698.588 1518.22 L698.588 1526.7 Q699.607 1526.35 700.625 1526.19 Q701.644 1526 702.662 1526 Q708.449 1526 711.829 1529.17 Q715.209 1532.34 715.209 1537.76 Q715.209 1543.34 711.736 1546.44 Q708.264 1549.52 701.945 1549.52 Q699.769 1549.52 697.5 1549.15 Q695.255 1548.78 692.848 1548.04 L692.848 1543.34 Q694.931 1544.47 697.153 1545.03 Q699.375 1545.58 701.852 1545.58 Q705.857 1545.58 708.195 1543.48 Q710.533 1541.37 710.533 1537.76 Q710.533 1534.15 708.195 1532.04 Q705.857 1529.94 701.852 1529.94 Q699.977 1529.94 698.102 1530.35 Q696.25 1530.77 694.306 1531.65 L694.306 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M1207.15 1544.91 L1214.79 1544.91 L1214.79 1518.55 L1206.48 1520.21 L1206.48 1515.95 L1214.75 1514.29 L1219.42 1514.29 L1219.42 1544.91 L1227.06 1544.91 L1227.06 1548.85 L1207.15 1548.85 L1207.15 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M1246.51 1517.37 Q1242.89 1517.37 1241.07 1520.93 Q1239.26 1524.47 1239.26 1531.6 Q1239.26 1538.71 1241.07 1542.27 Q1242.89 1545.82 1246.51 1545.82 Q1250.14 1545.82 1251.95 1542.27 Q1253.77 1538.71 1253.77 1531.6 Q1253.77 1524.47 1251.95 1520.93 Q1250.14 1517.37 1246.51 1517.37 M1246.51 1513.66 Q1252.32 1513.66 1255.37 1518.27 Q1258.45 1522.85 1258.45 1531.6 Q1258.45 1540.33 1255.37 1544.94 Q1252.32 1549.52 1246.51 1549.52 Q1240.7 1549.52 1237.62 1544.94 Q1234.56 1540.33 1234.56 1531.6 Q1234.56 1522.85 1237.62 1518.27 Q1240.7 1513.66 1246.51 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M1736.09 1544.91 L1743.73 1544.91 L1743.73 1518.55 L1735.42 1520.21 L1735.42 1515.95 L1743.68 1514.29 L1748.36 1514.29 L1748.36 1544.91 L1756 1544.91 L1756 1548.85 L1736.09 1548.85 L1736.09 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M1765.49 1514.29 L1783.84 1514.29 L1783.84 1518.22 L1769.77 1518.22 L1769.77 1526.7 Q1770.79 1526.35 1771.81 1526.19 Q1772.83 1526 1773.84 1526 Q1779.63 1526 1783.01 1529.17 Q1786.39 1532.34 1786.39 1537.76 Q1786.39 1543.34 1782.92 1546.44 Q1779.45 1549.52 1773.13 1549.52 Q1770.95 1549.52 1768.68 1549.15 Q1766.44 1548.78 1764.03 1548.04 L1764.03 1543.34 Q1766.11 1544.47 1768.34 1545.03 Q1770.56 1545.58 1773.03 1545.58 Q1777.04 1545.58 1779.38 1543.48 Q1781.71 1541.37 1781.71 1537.76 Q1781.71 1534.15 1779.38 1532.04 Q1777.04 1529.94 1773.03 1529.94 Q1771.16 1529.94 1769.28 1530.35 Q1767.43 1530.77 1765.49 1531.65 L1765.49 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M2268.12 1544.91 L2284.44 1544.91 L2284.44 1548.85 L2262.49 1548.85 L2262.49 1544.91 Q2265.15 1542.16 2269.74 1537.53 Q2274.34 1532.88 2275.52 1531.53 Q2277.77 1529.01 2278.65 1527.27 Q2279.55 1525.51 2279.55 1523.82 Q2279.55 1521.07 2277.61 1519.33 Q2275.69 1517.6 2272.58 1517.6 Q2270.39 1517.6 2267.93 1518.36 Q2265.5 1519.13 2262.72 1520.68 L2262.72 1515.95 Q2265.55 1514.82 2268 1514.24 Q2270.45 1513.66 2272.49 1513.66 Q2277.86 1513.66 2281.06 1516.35 Q2284.25 1519.03 2284.25 1523.52 Q2284.25 1525.65 2283.44 1527.57 Q2282.65 1529.47 2280.55 1532.07 Q2279.97 1532.74 2276.87 1535.95 Q2273.76 1539.15 2268.12 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M2304.25 1517.37 Q2300.64 1517.37 2298.81 1520.93 Q2297.01 1524.47 2297.01 1531.6 Q2297.01 1538.71 2298.81 1542.27 Q2300.64 1545.82 2304.25 1545.82 Q2307.88 1545.82 2309.69 1542.27 Q2311.52 1538.71 2311.52 1531.6 Q2311.52 1524.47 2309.69 1520.93 Q2307.88 1517.37 2304.25 1517.37 M2304.25 1513.66 Q2310.06 1513.66 2313.12 1518.27 Q2316.2 1522.85 2316.2 1531.6 Q2316.2 1540.33 2313.12 1544.94 Q2310.06 1549.52 2304.25 1549.52 Q2298.44 1549.52 2295.36 1544.94 Q2292.31 1540.33 2292.31 1531.6 Q2292.31 1522.85 2295.36 1518.27 Q2298.44 1513.66 2304.25 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip162)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,1451.23 2352.76,1451.23 \n  \"/>\n<polyline clip-path=\"url(#clip162)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,1224.71 2352.76,1224.71 \n  \"/>\n<polyline clip-path=\"url(#clip162)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,998.189 2352.76,998.189 \n  \"/>\n<polyline clip-path=\"url(#clip162)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,771.668 2352.76,771.668 \n  \"/>\n<polyline clip-path=\"url(#clip162)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,545.148 2352.76,545.148 \n  \"/>\n<polyline clip-path=\"url(#clip162)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  112.177,318.627 2352.76,318.627 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,1486.45 112.177,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,1451.23 131.075,1451.23 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,1224.71 131.075,1224.71 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,998.189 131.075,998.189 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,771.668 131.075,771.668 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,545.148 131.075,545.148 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  112.177,318.627 131.075,318.627 \n  \"/>\n<path clip-path=\"url(#clip160)\" d=\"M66.5939 1438.03 L54.7884 1456.47 L66.5939 1456.47 L66.5939 1438.03 M65.367 1433.95 L71.2466 1433.95 L71.2466 1456.47 L76.1772 1456.47 L76.1772 1460.36 L71.2466 1460.36 L71.2466 1468.51 L66.5939 1468.51 L66.5939 1460.36 L50.9921 1460.36 L50.9921 1455.85 L65.367 1433.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M55.2745 1207.43 L73.6309 1207.43 L73.6309 1211.37 L59.5569 1211.37 L59.5569 1219.84 Q60.5754 1219.49 61.5939 1219.33 Q62.6124 1219.14 63.6309 1219.14 Q69.418 1219.14 72.7976 1222.31 Q76.1772 1225.49 76.1772 1230.9 Q76.1772 1236.48 72.705 1239.58 Q69.2328 1242.66 62.9134 1242.66 Q60.7374 1242.66 58.4689 1242.29 Q56.2236 1241.92 53.8162 1241.18 L53.8162 1236.48 Q55.8995 1237.62 58.1217 1238.17 Q60.3439 1238.73 62.8208 1238.73 Q66.8254 1238.73 69.1633 1236.62 Q71.5013 1234.51 71.5013 1230.9 Q71.5013 1227.29 69.1633 1225.19 Q66.8254 1223.08 62.8208 1223.08 Q60.9458 1223.08 59.0708 1223.5 Q57.2189 1223.91 55.2745 1224.79 L55.2745 1207.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M64.6495 996.326 Q61.5013 996.326 59.6495 998.479 Q57.8208 1000.63 57.8208 1004.38 Q57.8208 1008.11 59.6495 1010.28 Q61.5013 1012.44 64.6495 1012.44 Q67.7976 1012.44 69.6263 1010.28 Q71.4781 1008.11 71.4781 1004.38 Q71.4781 1000.63 69.6263 998.479 Q67.7976 996.326 64.6495 996.326 M73.9318 981.673 L73.9318 985.933 Q72.1726 985.099 70.367 984.659 Q68.5846 984.22 66.8254 984.22 Q62.1958 984.22 59.7421 987.345 Q57.3115 990.47 56.9643 996.789 Q58.33 994.775 60.3902 993.71 Q62.4504 992.622 64.9272 992.622 Q70.1355 992.622 73.1448 995.794 Q76.1772 998.942 76.1772 1004.38 Q76.1772 1009.71 73.029 1012.92 Q69.8809 1016.14 64.6495 1016.14 Q58.6541 1016.14 55.4828 1011.56 Q52.3116 1006.95 52.3116 998.224 Q52.3116 990.03 56.2004 985.169 Q60.0893 980.284 66.6402 980.284 Q68.3994 980.284 70.1818 980.632 Q71.9874 980.979 73.9318 981.673 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M53.9551 754.388 L76.1772 754.388 L76.1772 756.379 L63.6309 788.948 L58.7467 788.948 L70.5522 758.324 L53.9551 758.324 L53.9551 754.388 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M64.3254 546.016 Q60.9921 546.016 59.0708 547.798 Q57.1726 549.58 57.1726 552.705 Q57.1726 555.83 59.0708 557.613 Q60.9921 559.395 64.3254 559.395 Q67.6587 559.395 69.58 557.613 Q71.5013 555.807 71.5013 552.705 Q71.5013 549.58 69.58 547.798 Q67.6819 546.016 64.3254 546.016 M59.6495 544.025 Q56.6402 543.284 54.9504 541.224 Q53.2838 539.164 53.2838 536.201 Q53.2838 532.057 56.2236 529.65 Q59.1865 527.243 64.3254 527.243 Q69.4874 527.243 72.4272 529.65 Q75.367 532.057 75.367 536.201 Q75.367 539.164 73.6772 541.224 Q72.0105 543.284 69.0244 544.025 Q72.404 544.812 74.279 547.104 Q76.1772 549.395 76.1772 552.705 Q76.1772 557.728 73.0985 560.414 Q70.0429 563.099 64.3254 563.099 Q58.6078 563.099 55.5291 560.414 Q52.4736 557.728 52.4736 552.705 Q52.4736 549.395 54.3717 547.104 Q56.2699 544.812 59.6495 544.025 M57.9365 536.641 Q57.9365 539.326 59.6032 540.83 Q61.293 542.335 64.3254 542.335 Q67.3346 542.335 69.0244 540.83 Q70.7374 539.326 70.7374 536.641 Q70.7374 533.955 69.0244 532.451 Q67.3346 530.946 64.3254 530.946 Q61.293 530.946 59.6032 532.451 Q57.9365 533.955 57.9365 536.641 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M54.5569 335.189 L54.5569 330.93 Q56.3162 331.763 58.1217 332.203 Q59.9273 332.643 61.6634 332.643 Q66.293 332.643 68.7235 329.541 Q71.1772 326.416 71.5244 320.073 Q70.1818 322.064 68.1217 323.129 Q66.0615 324.194 63.5615 324.194 Q58.3763 324.194 55.344 321.069 Q52.3347 317.92 52.3347 312.481 Q52.3347 307.157 55.4828 303.939 Q58.631 300.722 63.8624 300.722 Q69.8578 300.722 73.0059 305.328 Q76.1772 309.911 76.1772 318.661 Q76.1772 326.832 72.2883 331.717 Q68.4226 336.578 61.8717 336.578 Q60.1124 336.578 58.3069 336.231 Q56.5014 335.883 54.5569 335.189 M63.8624 320.536 Q67.0106 320.536 68.8393 318.383 Q70.6911 316.231 70.6911 312.481 Q70.6911 308.754 68.8393 306.601 Q67.0106 304.425 63.8624 304.425 Q60.7143 304.425 58.8625 306.601 Q57.0338 308.754 57.0338 312.481 Q57.0338 316.231 58.8625 318.383 Q60.7143 320.536 63.8624 320.536 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M1147.58 12.096 L1155.76 12.096 L1155.76 65.6895 L1185.21 65.6895 L1185.21 72.576 L1147.58 72.576 L1147.58 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M1209.6 32.4315 Q1203.6 32.4315 1200.12 37.1306 Q1196.64 41.7891 1196.64 49.9314 Q1196.64 58.0738 1200.08 62.7728 Q1203.56 67.4314 1209.6 67.4314 Q1215.55 67.4314 1219.04 62.7323 Q1222.52 58.0333 1222.52 49.9314 Q1222.52 41.8701 1219.04 37.1711 Q1215.55 32.4315 1209.6 32.4315 M1209.6 26.1121 Q1219.32 26.1121 1224.87 32.4315 Q1230.42 38.7509 1230.42 49.9314 Q1230.42 61.0714 1224.87 67.4314 Q1219.32 73.7508 1209.6 73.7508 Q1199.84 73.7508 1194.29 67.4314 Q1188.78 61.0714 1188.78 49.9314 Q1188.78 38.7509 1194.29 32.4315 Q1199.84 26.1121 1209.6 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M1271.7 28.5427 L1271.7 35.5912 Q1268.54 33.9709 1265.14 33.1607 Q1261.73 32.3505 1258.09 32.3505 Q1252.54 32.3505 1249.74 34.0519 Q1246.99 35.7533 1246.99 39.156 Q1246.99 41.7486 1248.97 43.2475 Q1250.96 44.7058 1256.95 46.0426 L1259.51 46.6097 Q1267.45 48.3111 1270.77 51.4303 Q1274.13 54.509 1274.13 60.0587 Q1274.13 66.3781 1269.11 70.0644 Q1264.12 73.7508 1255.37 73.7508 Q1251.73 73.7508 1247.76 73.0216 Q1243.83 72.3329 1239.45 70.9151 L1239.45 63.2184 Q1243.59 65.3654 1247.6 66.4591 Q1251.61 67.5124 1255.54 67.5124 Q1260.8 67.5124 1263.64 65.73 Q1266.47 63.9071 1266.47 60.6258 Q1266.47 57.5877 1264.41 55.9673 Q1262.38 54.3469 1255.46 52.8481 L1252.86 52.2405 Q1245.94 50.7821 1242.86 47.7845 Q1239.78 44.7463 1239.78 39.4801 Q1239.78 33.0797 1244.32 29.5959 Q1248.85 26.1121 1257.2 26.1121 Q1261.33 26.1121 1264.98 26.7198 Q1268.62 27.3274 1271.7 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M1314.92 28.5427 L1314.92 35.5912 Q1311.76 33.9709 1308.36 33.1607 Q1304.96 32.3505 1301.31 32.3505 Q1295.76 32.3505 1292.97 34.0519 Q1290.21 35.7533 1290.21 39.156 Q1290.21 41.7486 1292.2 43.2475 Q1294.18 44.7058 1300.18 46.0426 L1302.73 46.6097 Q1310.67 48.3111 1313.99 51.4303 Q1317.35 54.509 1317.35 60.0587 Q1317.35 66.3781 1312.33 70.0644 Q1307.35 73.7508 1298.6 73.7508 Q1294.95 73.7508 1290.98 73.0216 Q1287.05 72.3329 1282.68 70.9151 L1282.68 63.2184 Q1286.81 65.3654 1290.82 66.4591 Q1294.83 67.5124 1298.76 67.5124 Q1304.03 67.5124 1306.86 65.73 Q1309.7 63.9071 1309.7 60.6258 Q1309.7 57.5877 1307.63 55.9673 Q1305.61 54.3469 1298.68 52.8481 L1296.09 52.2405 Q1289.16 50.7821 1286.08 47.7845 Q1283 44.7463 1283 39.4801 Q1283 33.0797 1287.54 29.5959 Q1292.08 26.1121 1300.42 26.1121 Q1304.55 26.1121 1308.2 26.7198 Q1311.84 27.3274 1314.92 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip162)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  175.59,162.047 281.277,859.344 386.965,859.431 492.653,897.81 598.34,885.106 704.028,919.049 809.716,879.3 915.404,861.044 1021.09,908.972 1126.78,863.758 \n  1232.47,852.807 1338.15,910.278 1443.84,869.474 1549.53,897.776 1655.22,935.032 1760.9,812.553 1866.59,887.085 1972.28,937.383 2077.97,919.634 2183.66,872.794 \n  2289.34,805.152 \n  \"/>\n<polyline clip-path=\"url(#clip162)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  175.59,770.78 281.277,1438 386.965,1439.68 492.653,1437.39 598.34,1443.4 704.028,1446 809.716,1441.88 915.404,1440.55 1021.09,1439.55 1126.78,1442.6 \n  1232.47,1442.39 1338.15,1437.55 1443.84,1440.92 1549.53,1447.87 1655.22,1440.87 1760.9,1441.34 1866.59,1443.05 1972.28,1436.65 2077.97,1443.98 2183.66,1438.61 \n  2289.34,1444.46 \n  \"/>\n<path clip-path=\"url(#clip160)\" d=\"\nM1919.26 324.425 L2278.07 324.425 L2278.07 168.905 L1919.26 168.905  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1919.26,324.425 2278.07,324.425 2278.07,168.905 1919.26,168.905 1919.26,324.425 \n  \"/>\n<polyline clip-path=\"url(#clip160)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1944.15,220.745 2093.52,220.745 \n  \"/>\n<path clip-path=\"url(#clip160)\" d=\"M2118.42 203.465 L2147.65 203.465 L2147.65 207.4 L2135.39 207.4 L2135.39 238.025 L2130.69 238.025 L2130.69 207.4 L2118.42 207.4 L2118.42 203.465 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M2166.1 223.997 L2166.1 226.08 L2146.52 226.08 Q2146.8 230.478 2149.16 232.793 Q2151.54 235.085 2155.78 235.085 Q2158.23 235.085 2160.53 234.483 Q2162.84 233.881 2165.11 232.677 L2165.11 236.705 Q2162.82 237.677 2160.41 238.187 Q2158 238.696 2155.53 238.696 Q2149.32 238.696 2145.69 235.085 Q2142.08 231.474 2142.08 225.316 Q2142.08 218.951 2145.5 215.224 Q2148.95 211.474 2154.78 211.474 Q2160.02 211.474 2163.05 214.853 Q2166.1 218.21 2166.1 223.997 M2161.84 222.747 Q2161.8 219.252 2159.88 217.168 Q2157.98 215.085 2154.83 215.085 Q2151.27 215.085 2149.11 217.099 Q2146.98 219.113 2146.66 222.77 L2161.84 222.747 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M2189.62 212.863 L2189.62 216.891 Q2187.82 215.965 2185.87 215.502 Q2183.93 215.039 2181.84 215.039 Q2178.67 215.039 2177.08 216.011 Q2175.5 216.983 2175.5 218.928 Q2175.5 220.409 2176.64 221.265 Q2177.77 222.099 2181.2 222.863 L2182.65 223.187 Q2187.19 224.159 2189.09 225.941 Q2191.01 227.701 2191.01 230.872 Q2191.01 234.483 2188.14 236.589 Q2185.29 238.696 2180.29 238.696 Q2178.21 238.696 2175.94 238.279 Q2173.7 237.886 2171.2 237.076 L2171.2 232.677 Q2173.56 233.904 2175.85 234.529 Q2178.14 235.131 2180.39 235.131 Q2183.4 235.131 2185.02 234.113 Q2186.64 233.071 2186.64 231.196 Q2186.64 229.46 2185.46 228.534 Q2184.3 227.608 2180.34 226.752 L2178.86 226.404 Q2174.9 225.571 2173.14 223.858 Q2171.38 222.122 2171.38 219.113 Q2171.38 215.455 2173.97 213.465 Q2176.57 211.474 2181.34 211.474 Q2183.7 211.474 2185.78 211.821 Q2187.86 212.168 2189.62 212.863 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M2202.01 204.738 L2202.01 212.099 L2210.78 212.099 L2210.78 215.409 L2202.01 215.409 L2202.01 229.483 Q2202.01 232.654 2202.86 233.557 Q2203.74 234.46 2206.4 234.46 L2210.78 234.46 L2210.78 238.025 L2206.4 238.025 Q2201.47 238.025 2199.6 236.196 Q2197.72 234.344 2197.72 229.483 L2197.72 215.409 L2194.6 215.409 L2194.6 212.099 L2197.72 212.099 L2197.72 204.738 L2202.01 204.738 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip160)\" style=\"stroke:#e26f46; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1944.15,272.585 2093.52,272.585 \n  \"/>\n<path clip-path=\"url(#clip160)\" d=\"M2118.42 255.305 L2147.65 255.305 L2147.65 259.24 L2135.39 259.24 L2135.39 289.865 L2130.69 289.865 L2130.69 259.24 L2118.42 259.24 L2118.42 255.305 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M2160.04 267.92 Q2159.32 267.504 2158.46 267.318 Q2157.63 267.11 2156.61 267.11 Q2153 267.11 2151.06 269.471 Q2149.14 271.809 2149.14 276.207 L2149.14 289.865 L2144.85 289.865 L2144.85 263.939 L2149.14 263.939 L2149.14 267.967 Q2150.48 265.606 2152.63 264.471 Q2154.78 263.314 2157.86 263.314 Q2158.3 263.314 2158.84 263.383 Q2159.37 263.43 2160.02 263.545 L2160.04 267.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M2176.29 276.832 Q2171.13 276.832 2169.14 278.013 Q2167.15 279.193 2167.15 282.041 Q2167.15 284.309 2168.63 285.652 Q2170.13 286.971 2172.7 286.971 Q2176.24 286.971 2178.37 284.471 Q2180.52 281.948 2180.52 277.781 L2180.52 276.832 L2176.29 276.832 M2184.78 275.073 L2184.78 289.865 L2180.52 289.865 L2180.52 285.929 Q2179.07 288.291 2176.89 289.425 Q2174.71 290.536 2171.57 290.536 Q2167.59 290.536 2165.22 288.314 Q2162.89 286.068 2162.89 282.318 Q2162.89 277.943 2165.8 275.721 Q2168.74 273.499 2174.55 273.499 L2180.52 273.499 L2180.52 273.082 Q2180.52 270.143 2178.58 268.545 Q2176.66 266.925 2173.16 266.925 Q2170.94 266.925 2168.84 267.457 Q2166.73 267.99 2164.78 269.055 L2164.78 265.119 Q2167.12 264.217 2169.32 263.777 Q2171.52 263.314 2173.6 263.314 Q2179.23 263.314 2182.01 266.231 Q2184.78 269.147 2184.78 275.073 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M2193.56 263.939 L2197.82 263.939 L2197.82 289.865 L2193.56 289.865 L2193.56 263.939 M2193.56 253.846 L2197.82 253.846 L2197.82 259.24 L2193.56 259.24 L2193.56 253.846 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip160)\" d=\"M2228.28 274.217 L2228.28 289.865 L2224.02 289.865 L2224.02 274.355 Q2224.02 270.675 2222.58 268.846 Q2221.15 267.018 2218.28 267.018 Q2214.83 267.018 2212.84 269.217 Q2210.85 271.416 2210.85 275.212 L2210.85 289.865 L2206.57 289.865 L2206.57 263.939 L2210.85 263.939 L2210.85 267.967 Q2212.38 265.629 2214.44 264.471 Q2216.52 263.314 2219.23 263.314 Q2223.7 263.314 2225.99 266.092 Q2228.28 268.846 2228.28 274.217 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"Final Results | Train Loss: $(train_losses[end]) | Test Loss: $(test_losses[end])\")\n",
    "loss_plot = plot(0:length(test_losses)-1, [test_losses, train_losses], labels=[\"Test\" \"Train\"], title=\"Loss\")\n",
    "display(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ATYPE([1,1,2,2,1,1,1])\n",
    "state = nothing\n",
    "composition = []\n",
    "bars = 1\n",
    "while length(composition) < 10000\n",
    "    x, state = model(reshape(x, (1,7,1)), state=state)\n",
    "    append!(composition, [reshape(x.-1, (1, 7))])\n",
    "    if i2t[\"type\"][x[4]-1] == \"EOS\" break; end\n",
    "    if i2t[\"bar-beat\"][x[3]-1] == \"Bar\" bars+=1; end\n",
    "end\n",
    "# append!(composition, [reshape([0,0,0,0,0,0,0], (1, 7))])\n",
    "composition = vcat(composition...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CP in NP format.\n",
    "dir = \"out\"\n",
    "NPZ.npzwrite(\"$(dir)\\\\cp\\\\comp2.npz\", composition)\n",
    "\n",
    "# PyCall\n",
    "# using PyCall\n",
    "# pushfirst!(PyVector(pyimport(\"sys\").\"path\"), \"\")\n",
    "# write_midi = pyimport(\"write_midi\")\n",
    "# Load midi function.\n",
    "# Load CP in NP format.\n",
    "# Load word2event dictionary.\n",
    "# Call function (composition, src, word2event)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "07d7f1db1109769b3e76de940780c60d78e4999d8c8368b5454c1eb47431d439"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
